\documentclass{beamer}

% Imported Packages
\usepackage[utf8]{inputenc}
\usepackage{xcolor}

% Theme
\usetheme[white, compactlogo]{Wisconsin}
\setbeamertemplate{caption}[numbered]

%This block of code defines the information to appear in the
%Title page
\title{QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation}
\subtitle{arXiv:1806.10293, Kalashnikov et al, 2018.}
\author{\textit{Sumamrized by} Hyecheol (Jerry) Jang}
\institute{
  Department of Computer Sciences\\
  University of Wisconsin–Madison
}
\date{RL Paper Study, Jun. 29. 2020}
%End of title page configuration block
%------------------------------------------------------------


%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}
%------------------------------------------------------------


%---------------------------------------------------------
%This block defines the existing sections
\newcommand{\firstSec}{Motivation}
\newcommand{\secondSec}{Goal}
\newcommand{\thirdSec}{Overview of Model Architecture}

\newcommand{\refSec}{Bibliography}
% \newcommand{\nthSec}[n]{nth Section}
%---------------------------------------------------------


\begin{document}
  %The next statement creates the title page.
  \frame{\titlepage}

  % Section 1, Motivation
  \section{\firstSec}
    \begin{frame}
      \frametitle{\firstSec : Why Robotics + Reinforcement Learning}
      \begin{itemize}
        \item Usually, Robots are good at \textbf{repetitive tasks} (e.g. Assembly Line)
              \pause
        \item Want to make Robots that \textbf{identifies surroundings} and \textbf{behave accordingly},
              but it is difficult
              \pause
        \begin{itemize}
          \item \textbf{Deep Learning}\\
                Provide ability to handling real-world scenarios
          \item \textbf{Reinforcement Learning}\\
                Provide ability to make decision in long-term,
                using previous experiences in complex and robust scenarios
        \end{itemize}
        \pause
        \item Combining two techniques
        \begin{itemize}
          \item Able to learn policy continuously from their experience
          \item No need for manual engineering, use data they collects
        \end{itemize}
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{\firstSec : Difficulites of Using RL in Robotics}
      \begin{itemize}
        \item Varience in \textbf{visual and physical property of objects}
        \pause
        \begin{itemize}
          \item Hardness of object (Soft or Hard)
          \item Surface Characteristics (Slippery, Sticky, \ldots)
          \item Color Variation
          \item Shape Variation
          \item \ldots
        \end{itemize}
        \pause
        \item \textbf{Noise} of sensors
      \end{itemize}
      \pause
      \begin{itemize}
        \setlength{\itemindent}{.3in}
        \item[$\Rightarrow$] Still hard to handle though we have sufficiently large training set
        \begin{itemize}
          \setlength{\itemindent}{0.3in}
          \item[$\Rightarrow$] Collecting those training set is expensive (real experiments) 
        \end{itemize}
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{\firstSec : Previous Works}
      \begin{itemize}
        \item Focused on learning narrow, individual tasks
        \begin{itemize}
          \item hitting a ball
          \item opening door
          \item throwing objects
          \item \ldots
        \end{itemize}
        \pause
      \end{itemize}
      \begin{itemize}
        \setlength{\itemindent}{.3in}
        \item[$\Rightarrow$] Use \textbf{Grasping} to achieve \textit{generalization}
      \end{itemize}
      \pause
      \begin{itemize}
        \item Approached the grasping task as predicting a \textit{grasp pose}
        \begin{enumerate}
          \item Observe the scene (\textit{Normally, using a depth camera})
          \item Choose best location to grasp
          \item Reach the location (open-loop setting)
        \end{enumerate}
        \pause
        \begin{itemize}
          \item Different with how humans and animals behave
          \item Grasp is a \textbf{dynamical process} that sence and control at each stage
        \end{itemize}
      \end{itemize}
      \pause
      \begin{itemize}
        \setlength{\itemindent}{.3in}
        \item[$\Rightarrow$] \textbf{Where this researches start!!}
      \end{itemize}
    \end{frame}
  % End of Section 1


  % Section 2: Goal
  \section{\secondSec}
  \begin{frame}
    \frametitle{\secondSec}
    \centering
    \Large{Use Reinforcement Learning with Deep Neural Network\\
           to \textbf{perform pre-grasp manipulation}, \\
           \textbf{response to dynamic disturbances}, \\
           and \textbf{learn grasping in a generic framework} \\
           that makes minimal assumptions about the task}
  \end{frame}

  \begin{frame}
    \frametitle{\secondSec : Constraint/Condition + Literature Review}
    \begin{itemize}
      \item \textbf{Closed-loop condition} (With feedback, \textit{\scriptsize{Morrison, et al.}})
      \begin{itemize}
        \item For the other papers work on closed-loop grasping, they deals with servoing problems.
        \item This paper focuses on making generalized RL algorithm
        \item In practice, it makes Kalashnikov et al.'s method (this method)
              to autonomously acquire complicated grasping strategy
      \end{itemize}
      \pause
      \item \textbf{Self-supervised} learning task
      \begin{itemize}
        \item Compare to prevoius work(by Zeng et al.), Kalashnikov et al. utilize more general action space
        \item Actions consist of end-effector \textbf{Cartesian motion} and \textbf{gripper opening/closing}
      \end{itemize}
      \pause
      \item Observation comes from \textbf{a single RGB camera} over the sholder
      \begin{itemize}
        \item Many current grasping system utilizes depth sensing
        \item Using wrist-mounted cameras
      \end{itemize}
    \end{itemize}
  \end{frame}
  % End of Section 2

  % Section 3: Overview of Model Architecture
  \section{\thirdSec}
    \begin{frame}
      \frametitle{\thirdSec : MDP}
      \begin{itemize}
        \item General Formulation of Robotic Manipulation: \\
              Based on \textbf{Markov Decision Process (MDP)}
        \begin{itemize}
          \item partially observed formulation (POMDP) is more general.
          \item However, assuming current observation contains all necessary information for this task,
                it is sufficient to use MDP.
        \end{itemize}
        \pause
        \item MDP have a \textbf{general and powerful formalism} for decision making problems. \\
              However, it is \textbf{hard to train}
        \pause
        \item For each step of MDP:
        \begin{enumerate}
          \item Observes Image from robot's camera (see Fig. \ref{fig:WorkspaceOverview})
          \item choose a gripper command, Reward:
          \begin{itemize}
            \item failed grasp: reward of 0
            \item successful grasp: reward of 1 \\
                  Defined \textit{success} when the robot holds the object above a certain height
          \end{itemize}
        \end{enumerate}
        \pause
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{\thirdSec: MDP}
      \begin{figure}
        \begin{columns}
          \column{.4\linewidth}
            \centering
            \includegraphics[height=0.82\textheight]{Images/WorkspaceOverview.png}
          \column{.6\linewidth}
            \caption{Configuration of robot cell, with a sample observation image on top-right box}
            \label{fig:WorkspaceOverview}
        \end{columns}
      \end{figure}
    \end{frame}

    \begin{frame}
      \frametitle{\thirdSec: Algorithm Selection}
      \begin{itemize}
        \item Usually, Generalization needs diverse data
        \begin{itemize}
          \item However, recollecting experience on numerous objects after every policy update is impractical
          \item Reason for \textbf{not using on-policy algorithm}
        \end{itemize}
        \pause
        \item Using \textbf{scalable off-policy algorithm} based on Q-learning
        \begin{itemize}
          \item actor-critic algorithm are popular for handling continuous actions
          \item However, Kalashnikov et al. found \textbf{scalable and more stable ways} to train only Q-function
        \end{itemize}
        \pause
        \item Large Dataset and Network (See Fig. \ref{fig:ModelStructure})
        \begin{itemize}
          \item Kalashnikov et al. devised \textbf{distributed} training system (with 7 robots)
          \item \textbf{Asynchronously update} target values, collect \textbf{on-policy data}, \\
                reloads \textbf{off-policy data} from previous experiences, \\ 
                and train network on both data stream.
        \end{itemize}
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{\thirdSec : Algorithm Selection}
      \begin{figure}
        \centering
        \includegraphics[]{Images/ModelStructure.jpg}
        \caption{Distributed Reinforcement Learning infrastructure for QT-Opt.}
        \label{fig:ModelStructure}
      \end{figure}
    \end{frame}

    \begin{frame}
      \frametitle{Optional: On-policy vs Off-policy}
      \begin{itemize}
        \item \textbf{On-policy Learning} learns the value of the policy being \textbf{carried out by the agent}, including the exploration steps. \\
              e.g. SARSA(State-Action-Reward-State-Action) (See Fig. \ref{fig:SARSAAlgorithm})
        \item \textbf{Off-policy Learning} learns the value of the optimal policy \textbf{independently of the agent's action} \\
              e.g. Q-Learning (See Fig. \ref{fig:QLearningAlgorithm})
      \end{itemize}
    \end{frame}

    \begin{frame}
      \frametitle{Optional: On-policy vs Off-policy}
      \begin{columns}[b]
        \column{.5\linewidth}
          \centering
          \begin{figure}
            \includegraphics[height=.75\textheight]{Images/SARSA.png}
            \caption{SARSA Algorithm}
            \label{fig:SARSAAlgorithm}
          \end{figure}
         \column{.5\linewidth}
           \centering
           \begin{figure}
             \includegraphics[width=.9\linewidth]{Images/Q-Learning.png}
             \caption{Q-Learning Algorithm}
             \label{fig:QLearningAlgorithm}
           \end{figure}
      \end{columns}
    \end{frame}
  % End of Section 3

  % Endings: Bibliography
  \section{\refSec}
    \begin{frame}[allowframebreaks]
      \frametitle{\refSec}
      \begin{itemize}
        \item \textbf{Kalashnikov, Dmitry, et al. QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation. 28 Nov. 2018, arxiv.org/abs/1806.10293.}
        \item Irpan, Alex, and Peter Pastor. Scalable Deep Reinforcement Learning for Robotic Manipulation. 28 June 2018, ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html.
        \item Morrison, Douglas, et al. “Closing the Loop for Robotic Grasping: A Real-Time, Generative Grasp Synthesis Approach.” Robotics: Science and Systems XIV, 2018, doi:10.15607/rss.2018.xiv.021.
        \item Poole, David Lynton, and Alan K. Mackworth. Artificial Intelligence: Foundations of Computational Agents. Cmabridge University Press, 2018.
      \end{itemize}
    \end{frame}
  % End of Reference Lists

\end{document}
